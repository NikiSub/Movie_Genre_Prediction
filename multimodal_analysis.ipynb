{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "PyCharm (MultiModal-Movie-Genre-Analysis)",
      "language": "python",
      "name": "pycharm-90cbde0b"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "multimodal_analysis.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmzejPg62ikF"
      },
      "source": [
        "### Training Gated MultiModal Unit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pO3BWcty2ikF"
      },
      "source": [
        "###### load pretrained BERT model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7kzYtj9DLKlK",
        "outputId": "d4aaf41e-5e78-4c28-9330-6068de715304"
      },
      "source": [
        "!wget -nc http://www.cs.virginia.edu/~vicente/vislang/mmimdb-256.tar.gz\n",
        "!tar xf mmimdb-256.tar.gz -C ./data/mmimdb-256/"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-12-03 03:05:12--  http://www.cs.virginia.edu/~vicente/vislang/mmimdb-256.tar.gz\n",
            "Resolving www.cs.virginia.edu (www.cs.virginia.edu)... 128.143.67.11\n",
            "Connecting to www.cs.virginia.edu (www.cs.virginia.edu)|128.143.67.11|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 643479473 (614M) [application/x-gzip]\n",
            "Saving to: ‘mmimdb-256.tar.gz’\n",
            "\n",
            "mmimdb-256.tar.gz   100%[===================>] 613.67M   111MB/s    in 5.6s    \n",
            "\n",
            "2020-12-03 03:05:19 (109 MB/s) - ‘mmimdb-256.tar.gz’ saved [643479473/643479473]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UG8ZLHsZ2ikF"
      },
      "source": [
        "!pip install livelossplot --quiet\n",
        "!pip -q install transformers"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkbOF-ZV2ikF"
      },
      "source": [
        "import torch, os, json\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import BertTokenizer\n",
        "from transformers import BertForSequenceClassification\n",
        "from PIL import Image\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZE7wkB5CxUfW"
      },
      "source": [
        "### Load Pretrained Bert Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qRweBu7M2ikF",
        "outputId": "aa56f8f5-e615-4151-c347-7a5572a227e2"
      },
      "source": [
        "text_model = BertForSequenceClassification.from_pretrained('bert-base-uncased', \n",
        "    num_labels = 27,  output_attentions = False, \n",
        "    output_hidden_states = False)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "goWyCZ5m2ors",
        "outputId": "28adec04-cb39-4c29-9807-3ca5803375a6"
      },
      "source": [
        "text_model.cuda()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=27, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "llvYp8tg2ikG",
        "outputId": "5732bc53-d877-47a4-c183-e3c8eae611fc"
      },
      "source": [
        "text_model.load_state_dict(torch.load('best_model_bert.pth'))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGowlcQtxdGs"
      },
      "source": [
        "### Load Pretrained Resnet "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JpU5BjA1x_va"
      },
      "source": [
        "\n",
        "from torchvision import models\n",
        "\n",
        "image_model = models.resnet18(pretrained=True) \n",
        "image_model.cuda()\n",
        "num_ftrs = image_model.fc.in_features\n",
        "image_model.fc = nn.Linear(num_ftrs, 27)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EfjdqgBgIwgg",
        "outputId": "eb31d6ce-95e4-4793-f23a-a53dcf8fc50d"
      },
      "source": [
        "image_model.load_state_dict(torch.load('best_img_model.pth'))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBsfg7dAd9A5"
      },
      "source": [
        "Equations governing GMU are as follows: <br>\n",
        "$h_{v} = tanh(W_{v} \\cdot x_{})$ <br>\n",
        "$h_{t} = tanh(W_{t} \\cdot x_{t})$ <br>\n",
        "$z = \\sigma(W_{z} \\cdot[x_{v} \\cdot x_{t}])$ <br>\n",
        "$h = z \\ast h_{v} + (1 - z) \\ast h_{t}$ <br>\n",
        "$\\Theta = \\{W_{v}, W_{t}, W_{z}\\}$ <br>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d58fFE-6GrmD"
      },
      "source": [
        "\n",
        "\n",
        "# used to find h_v and h_t\n",
        "class LinearClassifier(nn.Module):  \n",
        "  def __init__(self, encoding_size):\n",
        "    super(LinearClassifier, self).__init__()\n",
        "    self.linear = nn.Linear(encoding_size, encoding_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.linear(x)\n",
        "    x = torch.tanh(x)\n",
        "    return x\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZnodpjBXdkLm"
      },
      "source": [
        "class LinearCombine(nn.Module):\n",
        "  def __init__(self, encoding_size):\n",
        "    super().__init__()\n",
        "    self.linear = nn.Linear(encoding_size, 27) # 27 is the number of genres\n",
        "\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "  def forward(self, x, y):\n",
        "    x = self.linear(torch.cat((x, y), dim=1))\n",
        "    x = self.sigmoid(x)\n",
        "    return x\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TPGQPOMg-q7"
      },
      "source": [
        "class Gated_MultiModal_Unit():\n",
        "\n",
        "  def __init__(self, img_model, text_model):\n",
        "    super().__init__()\n",
        "    self.bert_model = text_model\n",
        "    self.resnet_model = img_model\n",
        "    self.hv_gate = LinearClassifier(27) # num of categories\n",
        "    self.ht_gate = LinearClassifier(27) # num of categories\n",
        "    self.z_gate = LinearCombine(54) # 2* num of categories, for data fusion\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self, mode,imgs, texts, text_masks, labels):\n",
        "    if (mode == 'train'):\n",
        "      self.hv_gate.train()\n",
        "      self.ht_gate.train()\n",
        "      self.z_gate.train()\n",
        "\n",
        "    else:\n",
        "      self.hv_gate.eval()\n",
        "      self.ht_gate.eval()\n",
        "      self.z_gate.eval()\n",
        "\n",
        "    self.resnet_model.eval()\n",
        "    self.bert_model.eval()\n",
        "    self.resnet_model.cuda()\n",
        "    self.bert_model.cuda()\n",
        "    img_pred = self.resnet_model(imgs)\n",
        "    text_pred = self.bert_model(texts, text_masks).logits\n",
        "\n",
        "    \n",
        "    self.hv_gate.cuda()\n",
        "    self.ht_gate.cuda()\n",
        "    self.z_gate.cuda()\n",
        "\n",
        "\n",
        "    h_v = self.hv_gate(img_pred)\n",
        "    h_t = self.ht_gate(text_pred)\n",
        "\n",
        "    z = self.z_gate(img_pred, text_pred)\n",
        "\n",
        "    h = (torch.mul(z, h_v)) + (torch.mul((1-z), h_t))\n",
        "\n",
        "    predictions = h\n",
        "    return predictions\n",
        "\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t1pAD8y9LXCb",
        "outputId": "1d75c701-1683-4a21-ba7d-b6e32ea58f31"
      },
      "source": [
        "class MovieDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, folder = 'data/mmimdb-256/dataset-resized-256max', split = 'dev',\n",
        "                 image_transform = None):\n",
        "        self.json_dir = os.path.join(folder, split, 'metadata')\n",
        "        self.image_dir = os.path.join(folder, split, 'images')\n",
        "        self.image_transform = image_transform\n",
        "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "        # Category definitions of movies.\n",
        "        self.categories = ['Action', 'Adult', 'Adventure', 'Animation', 'Biography', \n",
        "                           'Comedy', 'Crime', 'Documentary', 'Drama', \n",
        "                           'Family', 'Fantasy', 'Film-Noir', 'History', \n",
        "                           'Horror', 'Music', 'Musical', 'Mystery', 'News', \n",
        "                           'Reality-TV', 'Romance', 'Sci-Fi', 'Short', \n",
        "                           'Sport', 'Talk-Show', 'Thriller', 'War', 'Western']\n",
        "        self.categories2ids = {category: id for (id, category) \n",
        "                               in enumerate(self.categories)}\n",
        "\n",
        "        # Load JSON files.\n",
        "        print('Loading %s ...' % self.json_dir, end = '')\n",
        "        fdir = os.listdir(self.json_dir)\n",
        "        self.metadata = [(fname[:-5], json.load(open(os.path.join(self.json_dir, fname)))) \n",
        "                     for fname in sorted(fdir) if not fname.startswith('.')]\n",
        "        print(' finished')\n",
        "        print(self.metadata)\n",
        "        # Pre-tokenizing all sentences.\n",
        "        print('Tokenizing...', end = '')\n",
        "        self.tokenized_plots = list()\n",
        "        for i in range(0, len(self.metadata)):\n",
        "            text = self.metadata[i][1]['plot'][0]\n",
        "            encoded_text = self.tokenizer.encode_plus(\n",
        "                text, add_special_tokens = True, truncation = True, \n",
        "                max_length = 256, padding = 'max_length',\n",
        "                return_attention_mask = True,\n",
        "                return_tensors = 'pt')\n",
        "            self.tokenized_plots.append(encoded_text)\n",
        "        print(' finished')\n",
        "            \n",
        "    def __getitem__(self, index: int):\n",
        "        # Load images on the fly.\n",
        "        filename, movie_data = self.metadata[index]\n",
        "        img_path = os.path.join(self.image_dir, filename + '.jpeg')\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        text = self.tokenized_plots[index]['input_ids'][0]\n",
        "        text_mask = self.tokenized_plots[index]['attention_mask'][0]\n",
        "        genres = movie_data['genres']\n",
        "\n",
        "        if self.image_transform: image = self.image_transform(image)\n",
        "\n",
        "        # Encode labels in a binary vector.\n",
        "        label_vector = torch.zeros((len(self.categories)))\n",
        "        label_ids = [self.categories2ids[cat] for cat in genres]\n",
        "        label_vector[label_ids] = 1\n",
        "\n",
        "        return image, text, text_mask, label_vector\n",
        "\n",
        "    def load_image_only(self, index: int):\n",
        "        filename, movie_data = self.metadata[index]\n",
        "        img_path = os.path.join(self.image_dir, filename + '.jpeg')\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        return image\n",
        "\n",
        "    def get_metadata(self, index: int):\n",
        "        _, movie_data = self.metadata[index]\n",
        "        return movie_data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.metadata)\n",
        "\n",
        "val_data = MovieDataset(split = 'dev')\n",
        "print('Data size: %d samples' % len(val_data))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading data/mmimdb-256/dataset-resized-256max/dev/metadata ..."
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " finished\n",
            "Data size: 2608 samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IzfE0W7sK4h0",
        "outputId": "bdfcb391-ce8e-4cc6-d72f-7ecd86b07562"
      },
      "source": [
        "# Let's setup the data loaders and preprocessing.\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Batch size is a liming factor on constrained resources.\n",
        "# only GPUs with a large memory can hold large batches.\n",
        "batch_size = 10\n",
        "\n",
        "image_transform = transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                             std=[0.229, 0.224, 0.225])])\n",
        "\n",
        "trainset = MovieDataset(split = 'train', image_transform = image_transform)\n",
        "valset = MovieDataset(split = 'dev', image_transform = image_transform)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(trainset, \n",
        "                                           batch_size = batch_size, \n",
        "                                           shuffle = True, \n",
        "                                           pin_memory = True,\n",
        "                                           num_workers = 4)\n",
        "val_loader = torch.utils.data.DataLoader(valset, \n",
        "                                         batch_size = batch_size, \n",
        "                                         shuffle = False)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading data/mmimdb-256/dataset-resized-256max/train/metadata ..."
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " finished\n",
            "Loading data/mmimdb-256/dataset-resized-256max/dev/metadata ... finished\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " finished\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fL0v8hMWMCgE"
      },
      "source": [
        "gmu = Gated_MultiModal_Unit(image_model, text_model)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kb1OlJaTbmIW"
      },
      "source": [
        "def top5_accuracy(predicted, labels):\n",
        "    sorted_vals, sorted_ids = \\\n",
        "        predicted.data.sigmoid().sort(dim = 1, descending = True)\n",
        "    pred_vals = sorted_vals[:, :5] > 0.5 # Anything with sigmoid > 0.5 is 1.\n",
        "    true_vals = labels.data.gather(1, sorted_ids[:, :5]) # Find true values.\n",
        "    return (pred_vals == true_vals).sum(dim = 1) / 5.0\n",
        "\n",
        "def f_score(predicted, labels):\n",
        "    #print(predicted)\n",
        "    #print(predicted.data.sigmoid())\n",
        "    pred_vals = predicted.data.sigmoid() > 0.5\n",
        "    #print(pred_vals)\n",
        "    #print(labels)\n",
        "    true_positive = ((pred_vals==1) & (pred_vals == labels)).sum(dim = 1)\n",
        "    false_positive = ((pred_vals==1) & (pred_vals != labels)).sum(dim = 1)\n",
        "    false_negative = ((pred_vals==0) & (pred_vals != labels)).sum(dim = 1)\n",
        "    f_score = true_positive/(true_positive+(false_positive+false_negative)/2)\n",
        "    return f_score"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvRJD5lBK5zJ"
      },
      "source": [
        "from transformers import AdamW  # optimizer that comes with this library.\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from livelossplot import PlotLosses\n",
        "import torch.nn as nn\n",
        "\n",
        "num_epochs = 4\n",
        "\n",
        "total_steps = len(train_loader) * num_epochs\n",
        "cost_function = nn.BCEWithLogitsLoss(reduction = 'none')\n",
        "best_accuracy = 0\n",
        "\n",
        "# Training Loop. \n",
        "def train_gmu(model, optimizer, criterion,num_epochs=4):\n",
        "  best_accuracy = 0\n",
        "  liveloss = PlotLosses(); current_step = 0\n",
        "  for epoch in range(0, num_epochs):\n",
        "      cumulative_accuracy = 0\n",
        "      cumulative_fscore = 0\n",
        "      cumulative_loss = 0\n",
        "      num_samples = 0\n",
        "      logs = {}\n",
        "      # model.train()\n",
        "      # training loop\n",
        "      for (batch_id, (imgs, texts, text_masks, labels)) in enumerate(train_loader):\n",
        "          optimizer.zero_grad()\n",
        "          # Move to GPU.\n",
        "          if(torch.cuda.is_available()):\n",
        "            imgs = imgs.cuda()\n",
        "            texts = texts.cuda()\n",
        "            text_masks = text_masks.cuda()\n",
        "            labels = labels.cuda()\n",
        "\n",
        "          # Compute predictions.\n",
        "          predicted = model.forward('train', imgs, texts, text_masks, labels)\n",
        "          loss = criterion(predicted, labels)\n",
        "          loss.mean().backward()\n",
        "          optimizer.step() \n",
        "          cumulative_accuracy += top5_accuracy(predicted, labels).sum().item()\n",
        "          cumulative_loss += loss.data.sum().item()\n",
        "          f_s = f_score(predicted, labels)\n",
        "          cumulative_fscore +=f_s.sum().item()\n",
        "          num_samples += texts.size(0)\n",
        "          # print(\"Multimodal Accuracy: \", cumulative_accuracy/num_samples)\n",
        "          # print(\"Multimodal Fscore: \", cumulative_fscore/num_samples)\n",
        "\n",
        "          if batch_id % 100 == 0:\n",
        "            print(epoch, batch_id, cumulative_accuracy / num_samples)\n",
        "            logs['loss'] = cumulative_loss / num_samples\n",
        "            logs['accuracy'] = cumulative_accuracy / num_samples\n",
        "            logs['f_score'] = cumulative_fscore/num_samples\n",
        "            liveloss.update(logs)\n",
        "            liveloss.send()\n",
        "            current_step += 1\n",
        "\n",
        "      # validation loop\n",
        "      cumulative_accuracy = 0\n",
        "      cumulative_fscore = 0\n",
        "      cumulative_loss = 0\n",
        "      num_samples = 0\n",
        "      for (batch_id, (imgs, texts, text_masks, labels)) in enumerate(train_loader):\n",
        "          optimizer.zero_grad()\n",
        "          # Move to GPU.\n",
        "          if(torch.cuda.is_available()):\n",
        "            imgs = imgs.cuda()\n",
        "            texts = texts.cuda()\n",
        "            text_masks = text_masks.cuda()\n",
        "            labels = labels.cuda()\n",
        "\n",
        "          # Compute predictions.\n",
        "          predicted = model.forward('validation', imgs, texts, text_masks, labels)\n",
        "          loss = criterion(predicted, labels)\n",
        "          cumulative_accuracy += top5_accuracy(predicted, labels).sum().item()\n",
        "          cumulative_loss += loss.data.sum().item()\n",
        "          f_s = f_score(predicted, labels)\n",
        "          cumulative_fscore +=f_s.sum().item()\n",
        "\n",
        "          num_samples += texts.size(0)\n",
        "          # print(\"Multimodal Accuracy: \", cumulative_accuracy/num_samples)\n",
        "          # print(\"Multimodal Fscore: \", cumulative_fscore/num_samples)\n",
        "          if (1 + batch_id) % 100 == 0:\n",
        "            logs['val_loss'] = cumulative_loss / num_samples\n",
        "            logs['val_accuracy'] = cumulative_accuracy / num_samples \n",
        "            logs['val_f_score']  = cumulative_fscore/num_samples\n",
        "            liveloss.update(logs, current_step)\n",
        "            liveloss.send()\n",
        "            current_step += 1\n",
        "\n",
        "\n",
        "      if logs['val_accuracy'] > best_accuracy:\n",
        "        best_accuracy = logs['val_accuracy']\n",
        "        torch.save({\n",
        "            'hv_gate_state_dict': model.hv_gate.state_dict(),\n",
        "            'ht_gate_state_dict': model.ht_gate.state_dict(),\n",
        "            'z_gate_state_dict': model.z_gate.state_dict(),\n",
        "            }, 'best_gmu.pth')\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 975
        },
        "id": "ofd12nHpWWo4",
        "outputId": "04779841-e94b-49fd-f929-deab2b922b49"
      },
      "source": [
        "from torch import optim \n",
        "\n",
        "learning_rate = 0.003\n",
        "\n",
        "optimizer = optim.Adam(list(gmu.ht_gate.parameters()) + list(gmu.z_gate.parameters()) + list(gmu.hv_gate.parameters()), lr=learning_rate)\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss(reduction = 'none')\n",
        "# scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0, num_training_steps = total_steps) \n",
        "\n",
        "\n",
        "# train_gmu(gmu, criterion, optimizer, scheduler, num_epochs)\n",
        "train_gmu(gmu, optimizer, criterion, 4)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x864 with 4 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "accuracy\n",
            "\taccuracy         \t (min:    0.800, max:    0.800, cur:    0.800)\n",
            "f_score\n",
            "\tf_score          \t (min:    0.447, max:    0.447, cur:    0.447)\n",
            "Loss\n",
            "\tloss             \t (min:   10.613, max:   10.613, cur:   10.613)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}